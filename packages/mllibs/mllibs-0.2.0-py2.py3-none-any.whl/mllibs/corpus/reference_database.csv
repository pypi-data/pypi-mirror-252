id,data
db0,empty
db1,"
import numpy as np
from sklearn.model_selection import train_test_split
X = np.arange(10).reshape((5, 2))
X

array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])

X_train, X_test = train_test_split(X, y, test_size=0.33, random_state=42)

X_train
array([[4, 5],
       [0, 1],
       [6, 7]])

X_test
array([[2, 3],
       [8, 9]])
"
db2,"
*arrays: sequence of indexables with same length / shape[0]
Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.

<b>test_size</b>: [float or int], default=None
If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.

<b>train_size</b>: [float or int], default=None
If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.

<b>random_state</b>: [int], RandomState instance or None, default=None
Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls. See Glossary.

<b>shuffle</b>: [bool], default=True
Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.

<b>stratify</b>: [array-like], default=None
If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide.
"
db3,"
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_score
diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]
lasso = linear_model.Lasso()
print(cross_val_score(lasso, X, y, cv=3))
"
db4,"
<b>estimator</b>: estimator object implementing ‘fit’
The object to use to fit the data.

<b>X</b>: [array-like of shape] (n_samples, n_features)
The data to fit. Can be for example a list, or an array.

<b>y</b>: [array-like of shape] (n_samples,) or (n_samples, n_outputs), default=None
The target variable to try to predict in the case of supervised learning.

<b>groups</b>: [array-like of shape] (n_samples,), default=None
Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a “Group” cv instance (e.g., GroupKFold).

<b>scoring</b>: [str or callable], default=None
A str (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) which should return only a single value.

Similar to cross_validate but only a single metric is permitted.
If None, the estimator’s default scorer (if available) is used.

<b>cv</b>: [int], cross-validation generator or an iterable, default=None
Determines the cross-validation splitting strategy. Possible inputs for cv are:

    None, to use the default 5-fold cross validation,
    int, to specify the number of folds in a (Stratified)KFold,
    CV splitter,
    An iterable that generates (train, test) splits as arrays of indices.


<b>fit_params</b>: [dict], default=None
Parameters to pass to the fit method of the estimator.
"
db5,"
from sklearn import datasets
import pandas as pd

# convert sklearn format to dataframe
def sklearn_to_df(sklearn_dataset):
    df = pd.DataFrame(sklearn_dataset.data, 
                      columns=sklearn_dataset.feature_names)
  
    df['target'] = pd.Series(sklearn_dataset.target)
    return df

# load dataset
data = sklearn_to_df(datasets.load_breast_cancer())
"
db6,"
from sklearn import datasets
import pandas as pd

# convert sklearn format to dataframe
def sklearn_to_df(sklearn_dataset):
    df = pd.DataFrame(sklearn_dataset.data, 
                      columns=sklearn_dataset.feature_names)
    
    df['target'] = pd.Series(sklearn_dataset.target)
    return df

# load dataset
data = sklearn_to_df(datasets.load_diabetes())
"
db7,"
Here is an example of using the sklearn pipeline:

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

# Create the pipeline
pipe = Pipeline([
    ('scaler', StandardScaler()),        # Data Scaling
    ('pca', PCA(n_components=2)),        # Dimensionality Reduction
    ('classifier', LogisticRegression()) # Classifier
])

# Fit the pipeline to the data
pipe.fit(X_train, y_train)

# Evaluate the pipeline on the test set
accuracy = pipe.score(X_test, y_test)
print(""Accuracy:"", accuracy)
"
db8,"
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Create the pipeline
pipe = Pipeline([
    ('vectorizer', CountVectorizer()),   # BoW Vectoriser
    ('classifier', MultinomialNB())      # Classifier
])

# Fit the pipeline to the data
pipe.fit(X_train, y_train)

# Evaluate the pipeline on the test set
accuracy = pipe.score(X_test, y_test)
print(""Accuracy:"", accuracy)
"
db9,"
decision_function(X) | Transform the data, and apply decision_function with the final estimator.
fit(X[, y] | Fit the model.
fit_predict(X[, y]) | Transform the data, and apply fit_predict with the final estimator.
fit_transform(X[, y]) | Fit the model and transform with the final estimator.
get_feature_names_out([input_features]) | Get output feature names for transformation.
get_metadata_routing() | Get metadata routing of this object.
get_params([deep]) | Get parameters for this estimator.
inverse_transform(Xt) | Apply inverse_transform for each step in a reverse order.
predict(X, **predict_params) | Transform the data, and apply predict with the final estimator.
predict_log_proba(X, **predict_log_proba_params) | Transform the data, and apply predict_log_proba with the final estimator.
predict_proba(X, **predict_proba_params) | Transform the data, and apply predict_proba with the final estimator.
score(X[, y, sample_weight]) | Transform the data, and apply score with the final estimator.
score_samples(X) | Transform the data, and apply score_samples with the final estimator.
set_output(*[, transform]) | Set the output container when ""transform"" and ""fit_transform"" are called.
set_params(**kwargs) | Set the parameters of this estimator.
set_score_request(*[, sample_weight]) | Request metadata passed to the score method.
transform(X) | Transform the data, and apply transform with the final estimator.
"
db10,"
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

data = {""id"": [1, 2, 3, 4, 5,],
	    ""fruit"": [""Apple"", ""Apple"", ""Peach"", ""Banana""]}
df = pd.DataFrame({k: pd.Series(v) for k, v in data.items()})

pipe = Pipeline(
	steps=[
		(""ohe"", FunctionTransformer(pd.get_dummies, 
			kw_args={""dummy_na"": True, 
					 ""dtype"": ""float""}),
		)]
)
transformed_df = pipe.fit_transform(df)
"
