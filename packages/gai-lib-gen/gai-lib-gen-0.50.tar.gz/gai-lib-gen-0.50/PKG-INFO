Metadata-Version: 2.1
Name: gai-lib-gen
Version: 0.50
Summary: Gai/Gen is the Universal Multi-Modal Wrapper Library for LLM. The library is designed to provide a simplified and unified interface for seamless switching between multi-modal open source language models on a local machine and OpenAI APIs.
Author: kakkoii1337
Author-email: kakkoii1337@gmail.com
Classifier: Programming Language :: Python :: 3.10
Classifier: Development Status :: 3 - Alpha
Classifier: License :: OSI Approved :: MIT License
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Provides-Extra: ttt
Requires-Dist: torch==2.1.2; extra == "ttt"
Requires-Dist: exllama==0.1.0; extra == "ttt"
Requires-Dist: llama_cpp_python==0.2.29; extra == "ttt"
Requires-Dist: openai==1.8.0; extra == "ttt"
Requires-Dist: anthropic==0.9.0; extra == "ttt"
Requires-Dist: torchaudio==2.1.2; extra == "ttt"
Requires-Dist: torchvision==0.16.2; extra == "ttt"
Requires-Dist: pillow==10.2.0; extra == "ttt"
Requires-Dist: transformers==4.36.2; extra == "ttt"
Requires-Dist: bitsandbytes==0.42.0; extra == "ttt"
Requires-Dist: accelerate==0.26.1; extra == "ttt"
Requires-Dist: huggingface-hub; extra == "ttt"
Requires-Dist: pydantic; extra == "ttt"
Requires-Dist: python-dotenv; extra == "ttt"
Requires-Dist: uvicorn; extra == "ttt"
Requires-Dist: fastapi; extra == "ttt"
Provides-Extra: itt
Requires-Dist: torch==2.0.1; extra == "itt"
Requires-Dist: torchvision==0.15.2; extra == "itt"
Requires-Dist: transformers==4.31.0; extra == "itt"
Requires-Dist: tokenizers<0.14,>=0.12.1; extra == "itt"
Requires-Dist: sentencepiece==0.1.99; extra == "itt"
Requires-Dist: shortuuid; extra == "itt"
Requires-Dist: accelerate==0.21.0; extra == "itt"
Requires-Dist: peft==0.4.0; extra == "itt"
Requires-Dist: bitsandbytes==0.41.0; extra == "itt"
Requires-Dist: pydantic<2,>=1; extra == "itt"
Requires-Dist: markdown2[all]; extra == "itt"
Requires-Dist: numpy; extra == "itt"
Requires-Dist: openai==1.6.1; extra == "itt"
Requires-Dist: python-dotenv; extra == "itt"
Requires-Dist: scikit-learn==1.2.2; extra == "itt"
Requires-Dist: gradio==3.35.2; extra == "itt"
Requires-Dist: gradio_client==0.2.9; extra == "itt"
Requires-Dist: requests; extra == "itt"
Requires-Dist: httpx==0.24.0; extra == "itt"
Requires-Dist: ipykernel==6.27.1; extra == "itt"
Requires-Dist: uvicorn; extra == "itt"
Requires-Dist: fastapi; extra == "itt"
Requires-Dist: einops==0.6.1; extra == "itt"
Requires-Dist: einops-exts==0.0.4; extra == "itt"
Requires-Dist: timm==0.6.13; extra == "itt"
Provides-Extra: itt2
Requires-Dist: pillow==10.2.0; extra == "itt2"
Requires-Dist: transformers==4.36.2; extra == "itt2"
Requires-Dist: torch==2.1.2; extra == "itt2"
Requires-Dist: accelerate==0.26.1; extra == "itt2"
Requires-Dist: bitsandbytes==0.42.0; extra == "itt2"
Requires-Dist: openai==1.6.1; extra == "itt2"
Provides-Extra: stt
Requires-Dist: accelerate==0.25.0; extra == "stt"
Requires-Dist: ipykernel==6.27.1; extra == "stt"
Requires-Dist: openai==1.6.1; extra == "stt"
Requires-Dist: python-dotenv==1.0.0; extra == "stt"
Requires-Dist: torch==2.1.2; extra == "stt"
Requires-Dist: torchaudio==2.1.2; extra == "stt"
Requires-Dist: torchvision==0.16.2; extra == "stt"
Requires-Dist: transformers==4.36.2; extra == "stt"
Requires-Dist: uvicorn==0.23.2; extra == "stt"
Requires-Dist: PyDub==0.25.1; extra == "stt"
Requires-Dist: python_multipart==0.0.6; extra == "stt"
Requires-Dist: fastapi; extra == "stt"
Provides-Extra: tts
Requires-Dist: torch==2.1.2; extra == "tts"
Requires-Dist: torchaudio==2.1.2; extra == "tts"
Requires-Dist: transformers==4.36.2; extra == "tts"
Requires-Dist: openai==1.6.1; extra == "tts"
Requires-Dist: python-dotenv==1.0.0; extra == "tts"
Requires-Dist: TTS==0.22.0; extra == "tts"
Requires-Dist: deepspeed==0.12.6; extra == "tts"
Requires-Dist: uvicorn==0.23.2; extra == "tts"
Requires-Dist: ninja==1.11.1.1; extra == "tts"
Requires-Dist: fastapi; extra == "tts"
Provides-Extra: rag
Requires-Dist: chromadb==0.4.22; extra == "rag"
Requires-Dist: uvicorn==0.23.2; extra == "rag"
Requires-Dist: InstructorEmbedding==1.0.1; extra == "rag"
Requires-Dist: PyMySQL==1.1.0; extra == "rag"
Requires-Dist: sentence_transformers==2.2.2; extra == "rag"
Requires-Dist: sentencepiece==0.1.99; extra == "rag"
Requires-Dist: langchain==0.1.0; extra == "rag"
Requires-Dist: fastapi; extra == "rag"
Requires-Dist: python-multipart; extra == "rag"
Requires-Dist: pandas; extra == "rag"

# Gai/Gen: Universal LLM Wrapper

Universal Multi-Modal Wrapper Library for LLM inferencing. The library provides a simplified and unified interface for seamless switching between multi-modal open source language models on a local machine and OpenAI APIs. This is intended for Developers who are targetting the use of multi-modal LLMs for both OpenAI API and local machine models.

## Table of Contents

-   [Gai/Gen: Universal LLM Wrapper](#gaigen-universal-llm-wrapper)
    -   [1. Introduction](#1-introduction)
    -   [2. Requirements](#2-requirements)
    -   [3. Credits](#3-credits)
    -   [4. Using Gai as a Library](#4-using-gai-as-a-library)
        -   [Configuration](#configuration)
        -   [API Key](#api-key)
        -   [Quick Start](#quick-start)
        -   [Examples](#examples)
    -   [5. Using Gai as a Service](#5-using-gai-as-a-service)
        -   [Endpoints](#endpoints)
        -   [Examples](#examples-1)
            -   [Text-to-Text Generation](/docs/TTT.ipynb)
            -   [Speech-to-Text Generation](/docs/STT.ipynb)
            -   [Text-to-Speech Generation](/docs/TTS.ipynb)
            -   [Image-to-Text Generation](/docs/ITT.ipynb)
            -   [Retrieval Augmented Generation](/docs/RAG.ipynb)

## 1. Introduction

The core object is called **Gaigen** - generative AI generator. The premise is for the code to run on as commonly available commodity hardware as possible. The main focus is on 7 billion parameters and below open source models. Gaigen is designed as singleton wrapper where only one model is loaded and cached into memory at any one time.

To avoid dependency conflicts, the wrappers are organised under the `gen` folder according to 5 mutually-exclusive categories:

-   ttt: Text-to-Text
-   tts: Text-to-Speech
-   stt: Speech-to-Text
-   itt: Image-to-Text
-   rag: Retrieval-Augmented Generation

## 2. Requirements

-   The instructions are tested mainly on:

    -   Windows 11 (22H2 Build 22621.3007) with WSL2 (5.12.133.1-microsoft-standard-WSL2)
    -   Ubuntu 20.04.2 LTS
    -   NVIDIA RTX 2060 GPU with 8GB VRAM. Run `nvidia-smi` to check if CUDA driver is installed.

    ![nvidia-smi](./docs/imgs/nvidia-smi.png)

    -   CUDA Toolkit 11.8 is required for the GPU accelerated models. Run `nvcc --version` to check if CUDA Toolkit is installed. Refer here https://gist.github.com/kakkoii1337/8a8d4d0bc71fa9c099a683d1601f219e if you need guidance.

## 3. Credits

This library is made possible by the generosity and hardwork of the following open source projects. You are highly encouraged to check out the original source and documentations.

TTT

-   [TheBloke](https://huggingface.co/TheBloke) for all the quantized models in the demo
-   [turboderp](https://github.com/turboderp/exllama) for ExLlama
-   Meta Team for the [LLaMa2](https://ai.meta.com/llama/) Model
-   HuggingFace team for the [Transformers](https://huggingface.co/docs/transformers/llm_tutorial) library and open source models
-   Mistral AI Team for [Mistral7B](https://mistral.ai/news/announcing-mistral-7b/) Model
-   Georgi Gerganov for [LLaMaCpp](https://github.com/ggerganov/llama.cpp)

ITT

-   Liu HaoTian for the [LLaVa](https://github.com/haotian-liu/LLaVA) Model and Library

TTS

-   [Coqui-AI](https://github.com/coqui-ai/TTS) for the xTTS Model

STT

-   [OpenAI](https://huggingface.co/openai/whisper-large-v3) for Open Sourcing Whisper v3

RAG

-   [chromadb](https://github.com/chroma-core/chroma) for AI-native open-source embedding database
-   [instructor](https://huggingface.co/hku-nlp/instructor-large) open source embedding model

## 4. Disclaimer

Maintainers of this repo are not responsible for the actions of third parties who use the models. Please consult an attorney before using models for commercial purposes.

## 5. License

This project is licensed under the [MIT](./LICENSE) License - see the LICENSE file for details.

---

## 6. Using Gai as a Library

Using Gai as a library requires you to install the right category of package but gives you more control over your interaction with Gaigen.
It is highly recommended that you install each category in separate virtual environments.

```bash
# Install library for text-to-text generation
pip install "gai-lib-gen[TTT]"

# Install library for text-to-speech generation
pip install "gai-lib-gen[TTS]"

# Install library for speech-to-text generation
pip install "gai-lib-gen[STT]"

# Install library for image-to-text generation (install LLaVA separately)
pip install "gai-lib-gen[ITT]"
git clone https://github.com/haotian-liu/LLaVA && cd LLaVA && pip install -e .

# Install library for retrieval-augmented generation
pip install "gai-lib[RAG]"
```

### 6.1 Configuration

**Step 1.** Create a `.gairc` file in your home directory. This file contains the default configurations for Gai.

```bash
{
    "app_dir": "~/gai"
}
```

**Step 2.** Create a `/gai` directory.

```bash
mkdir ~/gai
```

Copy `gai.json` from this repository into `~/gai`. This file contains the configurations for models and their respective loaders.

**Step 3.** Create `/gai/models` directory.

```bash
mkdir ~/gai/models
```

The final user directory structure looks like this:

```bash
home
├── gai
│   ├── gai.json
│   └── models
└── .gairc
```

### 6.2 Downloading Models

When downloading from huggingface model hub, it is recommended to use the [huggingface CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).
You will need to install the CLI first.

```sh
pip install huggingface-hub
```

To download a model, run the following command:

```sh
huggingface-cli download <repo-name>/<model-name> --local-dir ~/gai/models/<model-name> --local-dir-use-symlinks False
```

**Example:** Downloading the main branch

```sh
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \
                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \
                --local-dir-use-symlinks False
```

**Example:** Downloading 2 files

```sh
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \
                mistral-7b-instruct-v0.1.Q4_K_M.gguf  \
                config.json \
                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GGUF \
                --local-dir-use-symlinks False
```

### 6.3 API Key

-   All API keys should be stored in a `.env` file in the root directory of the project.  
    For example,

    ```.env
    OPENAI_API_KEY=<--replace-with-your-api-key-->
    ANTHROPIC_API_KEY=<--replace-with-your-api-key-->
    ```

### 6.4 Quick Start

**Step 1. Install virtal environment and Gai**

The following example shows how to install the TTT category but the same steps are applicable to the other categories as well.

```bash
conda create -n TTT python=3.10.10 -y
conda activate TTT
pip install gai-lib-gen[TTT]
```

**Step 2. Setup OpenAI API Key.**

Save your OpenAI API key in the **.env** file in the root directory of your project.

```bash
OPENAI_API_KEY=<--replace-with-your-api-key-->
```

**Step 3. Run Inferencing on GPT4.**

Run Text-to-Text generation using OpenAI by loading `gpt-4` wrapper.

```python
from gai.gen import Gaigen
gen = Gaigen.GetInstance().load('gpt-4')

response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}])
print(response)
```

**Step 4. Install Mistral7B.**

Download the model `Mistral-7B-Instruct-v0.1-GPTQ` into the `~/gai/models` folder.

```json
huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GPTQ \
                --local-dir ~/gai/models/Mistral-7B-Instruct-v0.1-GPTQ \
                --local-dir-use-symlinks False
```

**Step 5. Run Inferencing on Mistral**

Run Text-to-Text generation using Mistral7B by replacing `gpt-4` with `mistral7b-exllama`.

```python
from gai.gen import Gaigen
gen = Gaigen.GetInstance().load('mistral7b-exllama')

response = gen.create(messages=[{'role':'USER','content':'Tell me a one paragraph short story.'},{'role':'ASSISTANT','content':''}])
print(response)
```

## 7. Using Gai as a Service

Gai Service is meant to be a one-model-per-instance service. Unlike library, you cannot change the model during runtime.

The easiest to run Gai Service is to use a Docker container. You will need to download the models into ~/gai/models and map the volume to the container. You can then start up a container and post REST API calls to following endpoints.

### Endpoints

The following endpoints are only available for the category of models that you have installed.

**- Text-to-Text (TTT)**  
Endpoint: http://localhost:12031/gen/v1/chat/completions  
Method: POST  
Type: Body

| Name     | Type | Description                    | Default           |
| -------- | ---- | ------------------------------ | ----------------- |
| model    | str  | generator name                 | mistral7b-exllama |
| messages | list | See below                      |                   |
| stream   | bool | True, False                    | True              |
| ...      |      | Hyperparameters based on model |                   |

Note:

-   messages

```json
[
    { "role": "system", "content": system message },
    { "role": "user", "content": user message },
    { "role": "assistant", "content": AI message },
    ...
]
```

**- Text-to-Speech (TTS)**  
Endpoint: http://localhost:12031/gen/v1/audio/speech  
Method: POST  
Type: Body

| Name     | Type | Description                    | Default |
| -------- | ---- | ------------------------------ | ------- |
| model    | str  | generator name                 | xtts-2  |
| input    | str  | text to be spoken              |         |
| voice    | str  | voice id (speaker)             |         |
| language | file | language code                  | en      |
| stream   | bool | True, False                    | True    |
| ...      |      | Hyperparameters based on model |         |

**- Speech-to-Text**
Endpoint: http://localhost:12031/gen/v1/audio/transcriptions  
Method: POST  
Type: Multipart Form-Data

| Name  | Type | Description       | Default |
| ----- | ---- | ----------------- | ------- |
| model | str  | generator name    |         |
| file  | file | audio file object |         |

**- itt: Image-to-Text**
Endpoint: http://localhost:12031/gen/v1/vision/completions  
Method: POST
Type: Body
Parameters:

| Name     | Type | Description                    | Default |
| -------- | ---- | ------------------------------ | ------- |
| model    | str  | generator name                 |         |
| messages | list | see below                      |         |
| stream   | bool | True,False                     |         |
| ...      |      | Hyperparameters based on model |         |

Note:

-   messages format

```json
[
    {
        "role": "user",
        "content": [
            {"type": "text", "text": text},
            {
                "type": "image_url",
                "image_url": {
                    "url": 'data:image/jpeg;base64,.....',
                },
            },
        ],
        ...
    }
]
```

**- rag: Retrieval-Augmented Generation**

a) Endpoint: http://localhost:12031/gen/v1/rag/index_file  
Method: POST  
Type: Multipart Form-Data
Parameters:

| Name            | Type | Description                   | Default |
| --------------- | ---- | ----------------------------- | ------- |
| collection_name | str  | collection name in the store  |         |
| file            | file | the document to be indexed    |         |
| metadata        | dict | metadata tied to the document |         |

b) Endpoint: http://localhost:12031/gen/v1/rag/retrieve  
Method: POST  
Type: Body
Parameters:

| Name            | Type | Description                    | Default |
| --------------- | ---- | ------------------------------ | ------- |
| collection_name | str  | collection name in the store   |         |
| query_texts     | str  | query                          |         |
| n_results       | int  | no. of nearest result returned |         |

## 8. Examples

-   [Text-to-Text Generation (OpenAI GPT4 vs Open-Source Mistra7B)](/docs/TTT.ipynb)
-   [Speech-to-Text Generation (OpenAI Whisper vs Open-Source Whisper)](/docs/STT.ipynb)
-   [Text-to-Speech Generation (OpenAI Speech vs Open-Source xTTS)](/docs/TTS.ipynb)
-   [Image-to-Text Generation (OpenAI Vision vs Open-Source Llava)](/docs/ITT.ipynb)
-   [Retrieval Augmented Generation](/docs/RAG.ipynb)
